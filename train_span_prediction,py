import json
import os
import torch
from torch.utils.data import DataLoader
from transformers import (
    BertTokenizerFast, BertForQuestionAnswering, get_scheduler, DataCollatorWithPadding,
    AutoTokenizer, AutoModelForQuestionAnswering, default_data_collator
)
from accelerate import Accelerator
from tqdm.auto import tqdm
from collections import OrderedDict
import numpy as np

# Custom dataset class
class QAWithParagraphSelectionDataset(torch.utils.data.Dataset):
    def __init__(self, data, context_data, tokenizer, max_len):
        self.data = data
        self.context_data = context_data
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.preprocessed_data = self._preprocess()

    def _preprocess(self):
        preprocessed = []
        for idx, item in enumerate(tqdm(self.data, desc="Processing Dataset", unit="samples")):
            question = item['question']
            relevant_paragraph = self.context_data[item['relevant']].strip()
            answer_text = item['answer']['text']
            answer_start = item['answer']['start']

            # Tokenize question and relevant paragraph
            inputs = self.tokenizer(
                question,
                relevant_paragraph,
                truncation=True,
                max_length=self.max_len,
                padding="max_length",
                return_offsets_mapping=True,
                return_tensors="pt"
            )

            input_ids = inputs['input_ids'].squeeze(0)
            attention_mask = inputs['attention_mask'].squeeze(0)
            offset_mapping = inputs['offset_mapping'].squeeze(0)

            # Find start and end token positions for the answer
            start_position, end_position = None, None
            for idx_tok, (start, end) in enumerate(offset_mapping):
                if start <= answer_start < end:
                    start_position = idx_tok
                if start < answer_start + len(answer_text) <= end:
                    end_position = idx_tok
            if start_position is None or end_position is None:
                start_position, end_position = 0, 0

            preprocessed.append({
                'input_ids': input_ids,
                'attention_mask': attention_mask,
                'start_positions': torch.tensor(start_position, dtype=torch.long),
                'end_positions': torch.tensor(end_position, dtype=torch.long),
                'offset_mapping': offset_mapping,
                'context': relevant_paragraph,
                'question': question,
                'answer_text': answer_text,
                'example_id': idx  # Keep track of example IDs
            })

        return preprocessed

    def __len__(self):
        return len(self.preprocessed_data)

    def __getitem__(self, idx):
        return self.preprocessed_data[idx]

# Dataset loading function
def load_json(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

# Evaluation metrics
def compute_exact(a_gold, a_pred):
    return int(a_gold == a_pred)

def compute_f1(a_gold, a_pred):
    gold_tokens = a_gold.split()
    pred_tokens = a_pred.split()
    common = set(gold_tokens) & set(pred_tokens)
    if not common:
        return 0.0
    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(gold_tokens)
    f1 = 2 * (precision * recall) / (precision + recall)
    return f1

def train_and_evaluate(
        train_file, valid_file, context_file, model_name_or_path, output_dir, 
        learning_rate=3e-5, num_epochs=3, batch_size=4, max_len=512, mixed_precision="fp16"):
    
    # Load datasets
    train_data = load_json(train_file)
    valid_data = load_json(valid_file)
    context_data = load_json(context_file)
    
    # Load tokenizer and model
    tokenizer = BertTokenizerFast.from_pretrained(model_name_or_path)
    model = BertForQuestionAnswering.from_pretrained(model_name_or_path)
    
    # Create datasets
    train_dataset = QAWithParagraphSelectionDataset(train_data, context_data, tokenizer, max_len=max_len)
    valid_dataset = QAWithParagraphSelectionDataset(valid_data, context_data, tokenizer, max_len=max_len)
    
    # Initialize Accelerator
    accelerator = Accelerator(mixed_precision=mixed_precision)

    # Implement custom collate function
    def custom_collate_fn(features):
        batch = {}
        # Fields that need to be collated and padded
        tensor_fields = ['input_ids', 'attention_mask', 'start_positions', 'end_positions', 'offset_mapping']
        # Fields that are lists of strings or other data
        non_tensor_fields = ['context', 'question', 'answer_text', 'example_id']
        
        for field in tensor_fields:
            batch[field] = torch.stack([f[field] for f in features])
        
        for field in non_tensor_fields:
            batch[field] = [f[field] for f in features]
        
        return batch

    # Use the custom collate function
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)
    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)

    # Optimizer and Scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    num_training_steps = num_epochs * len(train_dataloader)
    lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=500, num_training_steps=num_training_steps)

    # Prepare model, optimizer, and data loaders
    model, optimizer, lr_scheduler = accelerator.prepare(model, optimizer, lr_scheduler)
    train_dataloader, valid_dataloader = accelerator.prepare(train_dataloader, valid_dataloader)

    # Training loop
    best_val_loss = float("inf")
    progress_bar = tqdm(range(num_training_steps))

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        for batch in train_dataloader:
            input_ids = batch['input_ids'].to(accelerator.device)
            attention_mask = batch['attention_mask'].to(accelerator.device)
            start_positions = batch['start_positions'].to(accelerator.device)
            end_positions = batch['end_positions'].to(accelerator.device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                start_positions=start_positions,
                end_positions=end_positions
            )
            loss = outputs.loss
            total_loss += loss.item()
            accelerator.backward(loss)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)

        avg_train_loss = total_loss / len(train_dataloader)
        print(f"Epoch {epoch+1} finished with average training loss: {avg_train_loss}")

        # Validation phase
        model.eval()
        total_val_loss = 0
        total_predictions = 0
        exact_match_total = 0
        f1_total = 0

        for batch in valid_dataloader:
            with torch.no_grad():
                input_ids = batch['input_ids'].to(accelerator.device)
                attention_mask = batch['attention_mask'].to(accelerator.device)
                start_positions = batch['start_positions'].to(accelerator.device)
                end_positions = batch['end_positions'].to(accelerator.device)

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    start_positions=start_positions,
                    end_positions=end_positions
                )
                val_loss = outputs.loss
                total_val_loss += val_loss.item()

                start_logits = outputs.start_logits.cpu().numpy()
                end_logits = outputs.end_logits.cpu().numpy()
                offset_mapping = batch['offset_mapping'].cpu().numpy()
                contexts = batch['context']  # Non-tensor fields
                answer_texts = batch['answer_text']

                for i in range(len(start_logits)):
                    # Get the best start and end logits positions
                    start_idx = np.argmax(start_logits[i])
                    end_idx = np.argmax(end_logits[i])

                    # Correction for end_idx less than start_idx
                    if end_idx < start_idx:
                        end_idx = start_idx

                    # Get the predicted answer text
                    offsets = offset_mapping[i]
                    start_char = offsets[start_idx][0]
                    end_char = offsets[end_idx][1]
                    predicted_answer = contexts[i][start_char:end_char]

                    # Get the ground truth answer text
                    ground_truth = answer_texts[i]

                    # Compute Exact Match and F1
                    exact_match = compute_exact(ground_truth, predicted_answer)
                    f1 = compute_f1(ground_truth, predicted_answer)

                    exact_match_total += exact_match
                    f1_total += f1
                    total_predictions += 1

        avg_val_loss = total_val_loss / len(valid_dataloader)
        exact_match_score = exact_match_total / total_predictions * 100
        f1_score = f1_total / total_predictions * 100

        print(f"Validation Loss after Epoch {epoch+1}: {avg_val_loss}")
        print(f"Exact Match Score: {exact_match_score:.2f}%")
        print(f"F1 Score: {f1_score:.2f}%")

        # Checkpointing if validation loss improves
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir)
            tokenizer.save_pretrained(output_dir)
            print(f"Best model saved with validation loss: {best_val_loss}")

        model.train()

# Example usage
train_and_evaluate(
    train_file='dataset/train.json',
    valid_file='dataset/valid.json',
    context_file='dataset/context.json',
    model_name_or_path="bert-base-chinese",
    output_dir="./span_prediction_model",
    learning_rate=3e-5,
    num_epochs=3,
    batch_size=2,
    max_len=512,
    mixed_precision="fp16"
)